{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee2fd93-3841-427c-b537-1d463c122fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:55:39.977758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c132f-556b-4d1e-a223-41f67b8c95cd",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e083c8ac-a437-4f75-a69c-19aa2b014fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./car_detection_dataset/train_bounding_boxes.csv')\n",
    "image_files = sorted([l for l in os.listdir('./car_detection_dataset/training_images') \n",
    "                      if l.lower().endswith(('.png', '.jpg', '.jpeg')) \n",
    "                      and os.path.isfile(os.path.join('./car_detection_dataset/training_images', l))])\n",
    "\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "os.makedirs('./car_detection_dataset/train/image', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/train/label', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/val/image', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/val/label', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7605cb4-5f5b-4242-8fb7-072f93ba6c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process and move training images\n",
    "for images in train_images:\n",
    "    source = os.path.join('./car_detection_dataset/training_images', images)\n",
    "    \n",
    "    # Copy image to the destination folder\n",
    "    destination = os.path.join('./car_detection_dataset/train/images', images)\n",
    "    shutil.copy(source, destination)\n",
    "    \n",
    "    # Retrieve bounding box data for the current image\n",
    "    image_data = df[df['image'] == images]\n",
    "    label_path = os.path.join('./car_detection_dataset/train/labels', f'{Path(images).stem}.txt')\n",
    "    \n",
    "    # Open the label file and write the bounding box annotations\n",
    "    with open(label_path, 'w') as lp:\n",
    "        if not image_data.empty:\n",
    "            image = cv2.imread(source)\n",
    "            for _, row in image_data.iterrows():\n",
    "                # Normalize the bounding box coordinates\n",
    "                x = (row['xmin'] + row['xmax']) / 2 / image.shape[1]\n",
    "                y = (row['ymin'] + row['ymax']) / 2 / image.shape[0]\n",
    "                w = (row['xmax'] - row['xmin']) / image.shape[1]\n",
    "                h = (row['ymax'] - row['ymin']) / image.shape[0]\n",
    "                \n",
    "                # Write the annotation (label) in YOLO format\n",
    "                lp.write(f'0 {x} {y} {w} {h}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a5c7c2-6679-4918-ab9c-2ebf2321e164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process and move validation images\n",
    "for images in val_images:\n",
    "    source = os.path.join('./car_detection_dataset/training_images', images)\n",
    "    # Copy image to the destination folder\n",
    "    destination = os.path.join('./car_detection_dataset/val/images', images)\n",
    "    shutil.copy(source, destination)\n",
    "    \n",
    "    # Retrieve bounding box data for the current image\n",
    "    image_data = df[df['image'] == images]\n",
    "    label_path = os.path.join('./car_detection_dataset/val/labels', f'{Path(images).stem}.txt')\n",
    "    \n",
    "    # Open the label file and write the bounding box annotations\n",
    "    with open(label_path, 'w') as lp:\n",
    "        if not image_data.empty:\n",
    "            image = cv2.imread(source)\n",
    "            for _, row in image_data.iterrows():\n",
    "                # Normalize the bounding box coordinates\n",
    "                x = (row['xmin'] + row['xmax']) / 2 / image.shape[1]\n",
    "                y = (row['ymin'] + row['ymax']) / 2 / image.shape[0]\n",
    "                w = (row['xmax'] - row['xmin']) / image.shape[1]\n",
    "                h = (row['ymax'] - row['ymin']) / image.shape[0]\n",
    "                \n",
    "                # Write the annotation (label) in YOLO format\n",
    "                lp.write(f'0 {x} {y} {w} {h}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9365d345-63d5-4b7e-9e6b-2f6b77b59138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_yaml = {\n",
    "    'train': './car_detection_dataset/train/images',\n",
    "    'val': './car_detection_dataset/val/images',\n",
    "    'nc': 1,  # Number of classes (in this case, only 'car')\n",
    "    'names': ['car']\n",
    "}\n",
    "\n",
    "# Save the YAML configuration to a file\n",
    "import yaml\n",
    "with open('data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bab656a-9b40-4c3c-9092-d5c246848186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Width: 676, Max Height: 380\n"
     ]
    }
   ],
   "source": [
    "maxWidth, maxHeight = 0, 0\n",
    "for name in os.listdir('./car_detection_dataset/training_images/'):\n",
    "    imagePath = os.path.join('./car_detection_dataset/training_images/', name)\n",
    "    # Read the image\n",
    "    image = cv2.imread(imagePath)  \n",
    "    if image is not None:  # Ensure the image was read successfully\n",
    "        height = image.shape[0]\n",
    "        width =  image.shape[1] # Get the dimensions\n",
    "        # Update max width and height if necessary\n",
    "        if width > maxWidth:\n",
    "            maxWidth = width\n",
    "        if height > maxHeight:\n",
    "            maxHeight = height\n",
    "\n",
    "# Output the maximum width and height\n",
    "print(f\"Max Width: {maxWidth}, Max Height: {maxHeight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34bf0157-9992-4358-80fb-2beb86ec817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP ðŸ’¡ Replace 'model=yolov5m.pt' with new 'model=yolov5mu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.3.41 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.39 ðŸš€ Python-3.12.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81038MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5m.pt, data=data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=car_detection_10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/car_detection_10\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      5280  ultralytics.nn.modules.conv.Conv             [3, 48, 6, 2, 2]              \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2     65280  ultralytics.nn.modules.block.C3              [96, 96, 2]                   \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    444672  ultralytics.nn.modules.block.C3              [192, 192, 4]                 \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  6   2512896  ultralytics.nn.modules.block.C3              [384, 384, 6]                 \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2]                 \n",
      "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
      " 10                  -1  1    295680  ultralytics.nn.modules.conv.Conv             [768, 384, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   1182720  ultralytics.nn.modules.block.C3              [768, 384, 2, False]          \n",
      " 14                  -1  1     74112  ultralytics.nn.modules.conv.Conv             [384, 192, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  2    296448  ultralytics.nn.modules.block.C3              [384, 192, 2, False]          \n",
      " 18                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  2   1035264  ultralytics.nn.modules.block.C3              [384, 384, 2, False]          \n",
      " 21                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2, False]          \n",
      " 24        [17, 20, 23]  1   4218643  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768]]          \n",
      "YOLOv5m summary: 339 layers, 25,065,715 parameters, 25,065,699 gradients, 64.4 GFLOPs\n",
      "\n",
      "Transferred 553/559 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/car_detection_10', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /blue/eel5934/r.anumula/project-3-graduate-RaviTejaAnumula/car_detection_dataset/train/labels.cache... 961 images, 625 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 961/961 [00:00<?, ?it/s]\n",
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /blue/eel5934/r.anumula/project-3-graduate-RaviTejaAnumula/car_detection_dataset/val/labels.cache... 362 images, 212 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 362/362 [00:00<?, ?it/s]\n",
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/car_detection_10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 91 weight(decay=0.0), 98 weight(decay=0.0005), 97 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/car_detection_10\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      7.07G       1.55      3.891      1.466          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:27<00:00,  2.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.111       0.83      0.117     0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      6.99G      1.581      1.308      1.607          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.897      0.859      0.928      0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      7.13G      1.593      1.145      1.694          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.894      0.838       0.92      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      7.03G      1.485      1.066      1.609          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.178      0.765      0.173     0.0991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      7.13G      1.395     0.8885       1.52          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.888      0.888      0.945      0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10       7.1G      1.433     0.8864      1.531          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.948      0.903      0.972      0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      7.08G      1.373     0.7914      1.512          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.932      0.925      0.974      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         7G      1.305     0.7805      1.391          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.948      0.975      0.988      0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      7.13G      1.293     0.7106      1.405          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241       0.95      0.938      0.986      0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      7.12G      1.248     0.6657      1.399          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.971      0.954      0.991      0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.039 hours.\n",
      "Optimizer stripped from runs/detect/car_detection_10/weights/last.pt, 50.5MB\n",
      "Optimizer stripped from runs/detect/car_detection_10/weights/best.pt, 50.5MB\n",
      "\n",
      "Validating runs/detect/car_detection_10/weights/best.pt...\n",
      "Ultralytics 8.3.39 ðŸš€ Python-3.12.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81038MiB)\n",
      "YOLOv5m summary (fused): 248 layers, 25,045,795 parameters, 0 gradients, 64.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241       0.95      0.938      0.986      0.664\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/car_detection_10\u001b[0m\n",
      "PRO TIP ðŸ’¡ Replace 'model=yolov5m.pt' with new 'model=yolov5mu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.3.41 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.39 ðŸš€ Python-3.12.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81038MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5m.pt, data=data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=car_detection_20, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/car_detection_20\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      5280  ultralytics.nn.modules.conv.Conv             [3, 48, 6, 2, 2]              \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2     65280  ultralytics.nn.modules.block.C3              [96, 96, 2]                   \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    444672  ultralytics.nn.modules.block.C3              [192, 192, 4]                 \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  6   2512896  ultralytics.nn.modules.block.C3              [384, 384, 6]                 \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2]                 \n",
      "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
      " 10                  -1  1    295680  ultralytics.nn.modules.conv.Conv             [768, 384, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   1182720  ultralytics.nn.modules.block.C3              [768, 384, 2, False]          \n",
      " 14                  -1  1     74112  ultralytics.nn.modules.conv.Conv             [384, 192, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  2    296448  ultralytics.nn.modules.block.C3              [384, 192, 2, False]          \n",
      " 18                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  2   1035264  ultralytics.nn.modules.block.C3              [384, 384, 2, False]          \n",
      " 21                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  2   4134912  ultralytics.nn.modules.block.C3              [768, 768, 2, False]          \n",
      " 24        [17, 20, 23]  1   4218643  ultralytics.nn.modules.head.Detect           [1, [192, 384, 768]]          \n",
      "YOLOv5m summary: 339 layers, 25,065,715 parameters, 25,065,699 gradients, 64.4 GFLOPs\n",
      "\n",
      "Transferred 553/559 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/car_detection_20', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /blue/eel5934/r.anumula/project-3-graduate-RaviTejaAnumula/car_detection_dataset/train/labels.cache... 961 images, 625 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 961/961 [00:00<?, ?it/s]\n",
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /blue/eel5934/r.anumula/project-3-graduate-RaviTejaAnumula/car_detection_dataset/val/labels.cache... 362 images, 212 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 362/362 [00:00<?, ?it/s]\n",
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/car_detection_20/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 91 weight(decay=0.0), 98 weight(decay=0.0005), 97 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/car_detection_20\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      7.12G      1.543      1.915      1.402          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:09<00:00,  6.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.846       0.89      0.924      0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      7.03G      1.424      1.087      1.416          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:09<00:00,  6.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.112      0.606     0.0947     0.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      7.14G      1.455      1.031      1.476          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241     0.0382      0.776     0.0357     0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      6.99G      1.426     0.9197      1.474          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.254      0.232     0.0931      0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      7.14G      1.371      0.826      1.426          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:09<00:00,  6.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.854      0.874      0.941      0.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      7.11G      1.365     0.7784      1.408          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.933      0.922      0.963      0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      7.09G      1.369     0.7968      1.395          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.936      0.964      0.987      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      7.03G      1.282     0.7111      1.342          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.975      0.952      0.989      0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      7.14G      1.269     0.7029       1.31          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  6.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.963      0.992       0.99      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20       7.1G      1.208     0.6465      1.286          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:09<00:00,  6.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.955      0.983       0.99      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/eel5934/r.anumula/.conda/envs/art/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20       7.1G      1.327     0.7113      1.415          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:10<00:00,  5.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241       0.95      0.948      0.988      0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         7G      1.263     0.7012      1.372          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.926      0.931      0.976      0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      7.14G      1.251     0.6748       1.38          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.971      0.975      0.991      0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      7.09G      1.237     0.6221      1.394          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.959      0.946      0.988      0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      7.08G      1.274     0.6249       1.38          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.975      0.979      0.993      0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      7.19G      1.223     0.5891      1.378          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241       0.96      0.984       0.99      0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      7.14G       1.21     0.5566      1.337          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.971      0.986      0.992      0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      7.09G      1.176     0.5698      1.331          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241       0.96      0.992      0.992      0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20       7.1G      1.147      0.528      1.348          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.971      0.987      0.993      0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      7.02G      1.136     0.5283      1.287          0        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:08<00:00,  7.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.972      0.996      0.993      0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.095 hours.\n",
      "Optimizer stripped from runs/detect/car_detection_20/weights/last.pt, 50.5MB\n",
      "Optimizer stripped from runs/detect/car_detection_20/weights/best.pt, 50.5MB\n",
      "\n",
      "Validating runs/detect/car_detection_20/weights/best.pt...\n",
      "Ultralytics 8.3.39 ðŸš€ Python-3.12.7 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81038MiB)\n",
      "YOLOv5m summary (fused): 248 layers, 25,045,795 parameters, 0 gradients, 64.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        362        241      0.972      0.996      0.993      0.703\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/car_detection_20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "pickle_file = 'model_info.pkl'\n",
    "dataset_yaml = 'data.yaml'\n",
    "results_csv = \"\"\n",
    "model_path = \"\"\n",
    "\n",
    "# Set batch sizes for training\n",
    "best_map = 0\n",
    "best_model_info = {}\n",
    "\n",
    "# Load dataset configuration from YAML file\n",
    "with open(dataset_yaml, 'r') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "\n",
    "    \n",
    "epoch_options = [10, 20]\n",
    "# Iterate over batch sizes to train and select the best model\n",
    "for epoch in epoch_options:\n",
    "    results_csv = f'./runs/detect/car_detection_{epoch}/results.csv'\n",
    "    model_path = f'./runs/detect/car_detection_{epoch}/weights/best.pt'\n",
    "    \n",
    "    if not os.path.exists(model_path):  # If model hasn't been trained\n",
    "        try:\n",
    "            # Determine image size (based on dataset)\n",
    "            max_width = max_height = 0\n",
    "            image = max(32 * round(max(max_width, max_height) / 32), 640)\n",
    "            \n",
    "            # Initialize YOLOv5m model and start training\n",
    "            model = YOLO('yolov5m.pt')\n",
    "            model.train(\n",
    "                data=dataset_yaml,\n",
    "                epochs=epoch,\n",
    "                imgsz=image,\n",
    "                batch=16,\n",
    "                name=f'car_detection_{epoch}'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training with batch size {epoch}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Read the results CSV after training\n",
    "    if os.path.exists(results_csv):\n",
    "        df = pd.read_csv(results_csv)\n",
    "        current_map = df['metrics/precision(B)'].iloc[-1]\n",
    "        \n",
    "        # Update best model if current mAP is higher\n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            best_model_info = {'model_path': model_path, 'results_csv': results_csv}\n",
    "    else:\n",
    "        print(f\"Results file for batch {epoch} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c99ce4f-7d0d-4176-88c6-66c05da50d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If a best model was found, save its information\n",
    "if best_model_info:\n",
    "    model_path = best_model_info['model_path']\n",
    "    results_csv = best_model_info['results_csv']\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(best_model_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b650637e-883a-4c54-bbb8-e688fc898788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning curves saved as 'learning-curves.png'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mAP achieved: 0.97215\n"
     ]
    }
   ],
   "source": [
    "# If best model info is available, plot learning curves\n",
    "# Assuming best_model_info is defined and df is loaded correctly\n",
    "if best_model_info:\n",
    "    df = pd.read_csv(best_model_info[\"results_csv\"])\n",
    "    metrics = ['train/box_loss', 'train/cls_loss', 'metrics/precision(B)', 'metrics/recall(B)']\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axs = axs.ravel()  # Flatten the 2D array to 1D for easier iteration\n",
    "    # Loop through each metric and plot it\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in df.columns:\n",
    "            axs[i].plot(df['epoch'], df[metric], label=metric)\n",
    "            axs[i].set_title(f'{metric} vs Epoch')\n",
    "            axs[i].set_xlabel('Epoch')\n",
    "            axs[i].set_ylabel('Value')\n",
    "            axs[i].legend(loc='upper right')\n",
    "        else:\n",
    "            axs[i].axis('off')  # Hide the axis if the metric is missing\n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    # Save figure as PNG\n",
    "    plt.savefig('learning-curves.png')\n",
    "    print(\"Learning curves saved as 'learning-curves.png'\")   \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "print(f\"Best mAP achieved: {best_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7736ce4-1651-4ef5-b930-9baf6b9feb3f",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfe5a8-bf8a-4a94-a8f7-a40548e57c0a",
   "metadata": {},
   "source": [
    "To assess how well the model performs without relying solely on labels, you can involve a small group of people for additional validation. Select a small set of test imagesâ€”say, 50 or 100â€”and show these images to a few individuals, asking them to mark areas where they see cars. Afterward, compare their annotations with the predictions made by your model. This will give you a clearer picture of how accurate your model's predictions are in real-world scenarios.\n",
    "\n",
    "For images without cars, you can train the model to recognize empty scenes as well. When testing, if the model doesn't detect any cars in an image, you can verify this by reviewing the image manually or asking your human helpers to confirm.\n",
    "\n",
    "This approach provides a practical way to gauge your modelâ€™s effectiveness, especially in cases where some images contain cars and others do not. It acts as a reality check, using human judgment to evaluate how well the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d087674-3ef1-433b-a334-facf9bee602f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./car_detection_dataset/train_bounding_boxes.csv')\n",
    "image_files = sorted([l for l in os.listdir('./car_detection_dataset/training_images') \n",
    "                      if l.lower().endswith(('.png', '.jpg', '.jpeg')) \n",
    "                      and os.path.isfile(os.path.join('./car_detection_dataset/training_images', l))])\n",
    "\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "os.makedirs('./car_detection_dataset/train_images_with_and_without_cars/images', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/train_images_with_and_without_cars/labels', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/val_images_with_and_without_cars/images', exist_ok=True)\n",
    "os.makedirs('./car_detection_dataset/val_images_with_and_without_cars/labels', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f11989-c0b5-43ac-b8b1-429d3a67aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process and move training images\n",
    "for images in train_images:\n",
    "    source = os.path.join('./car_detection_dataset/training_images', images)\n",
    "    \n",
    "    # Copy image to the destination folder\n",
    "    destination = os.path.join( './car_detection_dataset/train_images_with_and_without_cars', images)\n",
    "    shutil.copy(source, destination)\n",
    "    \n",
    "    # Retrieve bounding box data for the current image\n",
    "    image_data = df[df['image'] == images]\n",
    "    label_path = os.path.join('./car_detection_dataset/train_images_with_and_without_cars/labels', f'{Path(images).stem}.txt')\n",
    "    \n",
    "    # Open the label file and write the bounding box annotations\n",
    "    with open(label_path, 'w') as lp:\n",
    "        if not image_data.empty:\n",
    "            image = cv2.imread(source)\n",
    "            for _, row in image_data.iterrows():\n",
    "                # Normalize the bounding box coordinates\n",
    "                x = (row['xmin'] + row['xmax']) / 2 / image.shape[1]\n",
    "                y = (row['ymin'] + row['ymax']) / 2 / image.shape[0]\n",
    "                w = (row['xmax'] - row['xmin']) / image.shape[1]\n",
    "                h = (row['ymax'] - row['ymin']) / image.shape[0]\n",
    "                \n",
    "                # Write the annotation (label) in YOLO format\n",
    "                lp.write(f'0 {x} {y} {w} {h}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92fa13f9-336a-4bd3-aeaf-e8a01da8afc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process and move validation images\n",
    "for images in val_images:\n",
    "    source = os.path.join('./car_detection_dataset/training_images', images)\n",
    "    # Copy image to the destination folder\n",
    "    destination = os.path.join('./car_detection_dataset/val_images_with_and_without_cars', images)\n",
    "    shutil.copy(source, destination)\n",
    "    \n",
    "    # Retrieve bounding box data for the current image\n",
    "    image_data = df[df['image'] == images]\n",
    "    label_path = os.path.join('./car_detection_dataset/val_images_with_and_without_cars/labels', f'{Path(images).stem}.txt')\n",
    "    \n",
    "    # Open the label file and write the bounding box annotations\n",
    "    with open(label_path, 'w') as lp:\n",
    "        if not image_data.empty:\n",
    "            image = cv2.imread(source)\n",
    "            for _, row in image_data.iterrows():\n",
    "                # Normalize the bounding box coordinates\n",
    "                x = (row['xmin'] + row['xmax']) / 2 / image.shape[1]\n",
    "                y = (row['ymin'] + row['ymax']) / 2 / image.shape[0]\n",
    "                w = (row['xmax'] - row['xmin']) / image.shape[1]\n",
    "                h = (row['ymax'] - row['ymin']) / image.shape[0]\n",
    "                \n",
    "                # Write the annotation (label) in YOLO format\n",
    "                lp.write(f'0 {x} {y} {w} {h}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763fe555-d6af-48b5-b956-a04b3abcdab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_yaml = {\n",
    "    'train': './car_detection_dataset/train_images_with_and_without_cars/images',\n",
    "    'val': './car_detection_dataset/val_images_with_and_without_cars/images',\n",
    "    'nc': 1,  # Number of classes (in this case, only 'car')\n",
    "    'names': ['car']\n",
    "}\n",
    "\n",
    "# Save the YAML configuration to a file\n",
    "import yaml\n",
    "with open('dataset_with_and_without_cars.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a414fe-66e9-4f5b-9222-857899813318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Width: 676, Max Height: 380\n"
     ]
    }
   ],
   "source": [
    "maxWidth, maxHeight = 0, 0\n",
    "for name in os.listdir('./car_detection_dataset/training_images/'):\n",
    "    imagePath = os.path.join('./car_detection_dataset/training_images/', name)\n",
    "    # Read the image\n",
    "    image = cv2.imread(imagePath)  \n",
    "    if image is not None:  # Ensure the image was read successfully\n",
    "        height = image.shape[0]\n",
    "        width =  image.shape[1] # Get the dimensions\n",
    "        # Update max width and height if necessary\n",
    "        if width > maxWidth:\n",
    "            maxWidth = width\n",
    "        if height > maxHeight:\n",
    "            maxHeight = height\n",
    "\n",
    "# Output the maximum width and height\n",
    "print(f\"Max Width: {maxWidth}, Max Height: {maxHeight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84048d1-9a65-4187-bcb7-65470c29500c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP ðŸ’¡ Replace 'model=yolov5m.pt' with new 'model=yolov5mu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov5m.pt')\n",
    "imgsz = max(32 * round(max(maxWidth, maxHeight) / 32), 640)\n",
    "model_path = './runs/detect/car_detection_20/weights/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e816934-d069-4006-851f-6fc6ab48e872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(pred_boxes, true_boxes, image, iou_threshold=0.8, buffer=4):\n",
    "    cars_identified = 0\n",
    "    total_cars = len(true_boxes)\n",
    "    \n",
    "    img_h = image.shape[0]\n",
    "    img_w = image.shape[1]\n",
    "    \n",
    "    for true_box in true_boxes:\n",
    "        x, y, w, h = true_box\n",
    "        x1 = int((x - w/2) * img_w) \n",
    "        y1 = int((y - h/2) * img_h)\n",
    "        x2 = int((x + w/2) * img_w)\n",
    "        y2 = int((y + h/2) * img_h)\n",
    "        true_box_coords = [x1, y1, x2, y2]\n",
    "        \n",
    "        b2 = [x1 - buffer, y1 - buffer, x2 + buffer, y2 + buffer]\n",
    "        for pred_box in pred_boxes:\n",
    "            pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "            \n",
    "            # Calculate intersection area\n",
    "            inter_x1 = max(pred_x1, b2[0])\n",
    "            inter_y1 = max(pred_y1, b2[1])\n",
    "            inter_x2 = min(pred_x2, b2[2])\n",
    "            inter_y2 = min(pred_y2, b2[3])\n",
    "            \n",
    "            inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "            pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "            true_area = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
    "            \n",
    "            iou = inter_area / float(pred_area + true_area - inter_area)\n",
    "            \n",
    "            if iou > iou_threshold:\n",
    "                cars_identified += 1\n",
    "                break\n",
    "    return cars_identified, total_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb31fe2-f934-4754-881a-a91e3a38be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars identified: 1\n",
      "Total cars: 12\n",
      "Accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "def load_true_boxes(label_path):\n",
    "    \"\"\"Load ground truth bounding boxes from a label file.\"\"\"\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            return [list(map(float, line.strip().split()[1:])) for line in f]\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "def evaluate_and_accumulate(image_path, label_path, model, iou_threshold=0.8, buffer=4):\n",
    "    \"\"\"Evaluate predictions on a single image and accumulate identified and total car counts.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    predictions = model(image_path, verbose=False)\n",
    "    pred_boxes = predictions[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "    true_boxes = load_true_boxes(label_path)\n",
    "    \n",
    "    cars_identified, cars_in_image = evaluate_predictions(pred_boxes, true_boxes, image, iou_threshold, buffer)\n",
    "    \n",
    "    return cars_identified, cars_in_image\n",
    "\n",
    "def calculate_accuracy(total_identified, total_actual):\n",
    "    \"\"\"Calculate accuracy based on total identified and total actual cars.\"\"\"\n",
    "    if total_actual > 0:\n",
    "        return total_identified / total_actual\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def main(image_dir, label_dir, model_path, iou_threshold=0.8, buffer=4):\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    total_cars_identified = 0\n",
    "    total_cars = 0\n",
    "\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if not image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(image_file)[0] + '.txt')\n",
    "        \n",
    "        cars_identified, cars_in_image = evaluate_and_accumulate(image_path, label_path, model, iou_threshold, buffer)\n",
    "        \n",
    "        total_cars_identified += cars_identified\n",
    "        total_cars += cars_in_image\n",
    "    \n",
    "    accuracy = calculate_accuracy(total_cars_identified, total_cars)\n",
    "    \n",
    "    print(f\"Cars identified: {total_cars_identified}\")\n",
    "    print(f\"Total cars: {total_cars}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Define directories and model path\n",
    "image_dir = './Make sense/'\n",
    "label_dir = './Make sense labels/'\n",
    "model_path = './runs/detect/car_detection_20/weights/best.pt'\n",
    "\n",
    "# Run the evaluation\n",
    "main(image_dir, label_dir, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552e0be-dae2-4c1a-af67-dd8b41723c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "art"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
